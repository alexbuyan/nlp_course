{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlu-s2k9D1Ba"
      },
      "source": [
        "### Homework 5: Question search engine\n",
        "\n",
        "Remeber week01 where you used GloVe embeddings to find related questions? That was.. cute, but far from state of the art. It's time to really solve this task using context-aware embeddings.\n",
        "\n",
        "__Warning:__ this task assumes you have seen `seminar.ipynb`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HYffoHiI8du5",
        "outputId": "81b63fb3-0f4a-4f8a-81a0-01551eb064af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.10.13)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade transformers datasets accelerate deepspeed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfSHyQlT-fVF"
      },
      "source": [
        "### Load data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2_wgtrx8e6C",
        "outputId": "6864d201-54d1-42c5-f581-299f51543f82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Sample[0]: {'text1': 'How is the life of a math student? Could you describe your own experiences?', 'text2': 'Which level of prepration is enough for the exam jlpt5?', 'label': 0, 'idx': 0, 'label_text': 'not duplicate'}\n",
            "Sample[3]: {'text1': 'What can one do after MBBS?', 'text2': 'What do i do after my MBBS ?', 'label': 1, 'idx': 3, 'label_text': 'duplicate'}\n"
          ]
        }
      ],
      "source": [
        "qqp = datasets.load_dataset('SetFit/qqp')\n",
        "print('\\n')\n",
        "print(\"Sample[0]:\", qqp['train'][0])\n",
        "print(\"Sample[3]:\", qqp['train'][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pStlWcvD8rdk"
      },
      "outputs": [],
      "source": [
        "model_name = \"gchhablani/bert-base-cased-finetuned-qqp\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM3ZujeZ-Z7E"
      },
      "source": [
        "### Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qtkllSPG9bTL"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 128\n",
        "def preprocess_function(examples):\n",
        "    result = tokenizer(\n",
        "        examples['text1'], examples['text2'],\n",
        "        padding='max_length', max_length=MAX_LENGTH, truncation=True\n",
        "    )\n",
        "    result['label'] = examples['label']\n",
        "    return result\n",
        "\n",
        "qqp_preprocessed = qqp.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObMcFN59_Ll2",
        "outputId": "a9cfe3f2-8ef5-4f72-f045-0676a678a4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1731, 1110, 1103, 1297, 1104, 170, 12523, 2377, 136, 7426, 1128, 5594, 1240, 1319, 5758, 136,  ...\n"
          ]
        }
      ],
      "source": [
        "print(repr(qqp_preprocessed['train'][0]['input_ids'])[:100], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyQ1ZbzGAUF2"
      },
      "source": [
        "### Task 1: evaluation (1 point)\n",
        "\n",
        "We randomly chose a model trained on QQP - but is it any good?\n",
        "\n",
        "One way to measure this is with validation accuracy - which is what you will implement next.\n",
        "\n",
        "Here's the interface to help you do that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M5ueSoieAbBg"
      },
      "outputs": [],
      "source": [
        "val_set = qqp_preprocessed['validation']\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=1, shuffle=False, collate_fn=transformers.default_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsPwXXx-At-i",
        "outputId": "4ed14002-cc36-4af0-cc88-74053103b111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample batch: {'labels': tensor([0]), 'idx': tensor([0]), 'input_ids': tensor([[  101,  2009,  1132,  2170,   118,  4038,  1177,  2712,   136,   102,\n",
            "          2009,  1132,  1117, 10224,  4724,  1177,  2712,   136,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "\n",
            "Prediction (probs): [[9.998927e-01 1.072810e-04]]\n"
          ]
        }
      ],
      "source": [
        "for batch in val_loader:\n",
        "     break  # here be your training code\n",
        "print(\"Sample batch:\", batch)\n",
        "\n",
        "with torch.no_grad():\n",
        "  predicted = model(\n",
        "      input_ids=batch['input_ids'],\n",
        "      attention_mask=batch['attention_mask'],\n",
        "      token_type_ids=batch['token_type_ids']\n",
        "  )\n",
        "\n",
        "print('\\nPrediction (probs):', torch.softmax(predicted.logits, dim=1).data.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoxHzxn0DQqO"
      },
      "source": [
        "__Your task__ is to measure the validation accuracy of your model.\n",
        "Doing so naively may take several hours. Please make sure you use the following optimizations:\n",
        "\n",
        "- run the model on GPU with no_grad\n",
        "- using batch size larger than 1\n",
        "- use optimize data loader with num_workers > 1\n",
        "- (optional) use [mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "mcmfxY2S8lr4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "PLUm5QwP8ApQ",
        "outputId": "daeebf5a-3631-4cb8-bb0c-715e779c67df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "NWn_Iwh19jGZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9k5EK7-KA5F2",
        "outputId": "8664da0c-4ac1-4590-8acb-aaf6b2b1da2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 2527/2527 [05:02<00:00,  8.35it/s]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=16, shuffle=False, collate_fn=transformers.default_data_collator, num_workers=5\n",
        ")\n",
        "preds = []\n",
        "trues = []\n",
        "for batch in tqdm(val_loader):\n",
        "    batch_on_dev = {k: v.to(device) for k, v in batch.items()}\n",
        "    pred = model(\n",
        "        input_ids=batch_on_dev['input_ids'],\n",
        "        attention_mask=batch_on_dev['attention_mask'],\n",
        "        token_type_ids=batch_on_dev['token_type_ids']\n",
        "    )\n",
        "    preds_on_dev = torch.argmax(pred.logits, dim=-1).data\n",
        "    trues_on_dev = batch_on_dev['labels'].data\n",
        "    preds.extend(preds_on_dev.cpu().numpy())\n",
        "    trues.extend(trues_on_dev.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(trues, preds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert 0.9 < accuracy < 0.91\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R2z_-FZU3qy",
        "outputId": "d80b2524-752a-4016-fa73-83f998d5572c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9083848627256987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KONQ1E0J-y6B"
      },
      "source": [
        "### Task 2: train the model (4 points)\n",
        "\n",
        "For this task, you have two options:\n",
        "\n",
        "__Option A:__ fine-tune your own model. You are free to choose any model __except for the original BERT.__ We recommend [DeBERTa-v3](https://huggingface.co/microsoft/deberta-v3-base). Better yet, choose the best model based on public benchmarks (e.g. [GLUE](https://gluebenchmark.com/)).\n",
        "\n",
        "You can write the training code manually or use transformers.Trainer (see [this example](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification)). Please make sure that your model's accuracy is at least __comparable__ with the above example for BERT.\n",
        "\n",
        "\n",
        "__Option B:__ compare at least 3 pre-finetuned models (in addition to the above BERT model). For each model, report (1) its accuracy, (2) its speed, measured in samples per second in your hardware setup and (3) its size in megabytes. Please take care to compare models in equal setting, e.g. same CPU / GPU. Compile your results into a table and write a short (~half-page on top of a table) report, summarizing your findings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "iY0Rn_5fVFEt",
        "outputId": "98f06787-6f5e-47ff-d86b-f19d0a7a3a3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T0ZkZTkl_yMU",
        "outputId": "7df3ea98-7675-48d7-d958-f529f5cc93a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'microsoft/deberta-v3-base'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XpC8Uf82Lqxd",
        "outputId": "abfdbf42-85ac-438c-f6aa-ae6c85889fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d1188eeb38ff4bebb8900553aa064cba",
            "ffa4589b1e874bc0a8d866581b5b2b03",
            "76f378aad1c343cbb7ff6f7c3a1c45a7",
            "b1cda4f37973402db7ebd2af8771a715",
            "69a27955d12a41bca267133b2349b700",
            "2fdec81aa0b740b09976f2075b7ff8b4",
            "5db1001a2c0a40d7a41c26f0539d5854",
            "32359fae80aa4373afa73d87d4220c2e",
            "a4651eccdeb14299add5325d6dc2da87",
            "566c9622bf144a85b2d2adb6e02e22ca",
            "fa6c3f15d17243ffac5f224cd713ba5f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/390965 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1188eeb38ff4bebb8900553aa064cba"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "MAX_LENGTH = 128\n",
        "def preprocess_function(examples):\n",
        "    result = tokenizer(\n",
        "        examples['text1'], examples['text2'],\n",
        "        padding='max_length', max_length=MAX_LENGTH, truncation=True\n",
        "    )\n",
        "    result['label'] = examples['label']\n",
        "    return result\n",
        "\n",
        "qqp_preprocessed = qqp.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "6ka9qldAYtsx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ],
      "metadata": {
        "id": "UZf-69hC08GQ",
        "outputId": "d6746292-afed-4741-b4b9-3a89d9ea9b70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   2306 MiB |   4353 MiB |   5575 GiB |   5573 GiB |\n",
            "|       from large pool |   2305 MiB |   4351 MiB |   5390 GiB |   5388 GiB |\n",
            "|       from small pool |      1 MiB |      2 MiB |    184 GiB |    184 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   2306 MiB |   4353 MiB |   5575 GiB |   5573 GiB |\n",
            "|       from large pool |   2305 MiB |   4351 MiB |   5390 GiB |   5388 GiB |\n",
            "|       from small pool |      1 MiB |      2 MiB |    184 GiB |    184 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |   2269 MiB |   4352 MiB |   5575 GiB |   5573 GiB |\n",
            "|       from large pool |   2268 MiB |   4350 MiB |   5390 GiB |   5388 GiB |\n",
            "|       from small pool |      1 MiB |      2 MiB |    184 GiB |    184 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   4926 MiB |   4926 MiB |   4926 MiB |      0 B   |\n",
            "|       from large pool |   4922 MiB |   4922 MiB |   4922 MiB |      0 B   |\n",
            "|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 230565 KiB | 421515 KiB |   3558 GiB |   3558 GiB |\n",
            "|       from large pool | 229940 KiB | 421300 KiB |   3372 GiB |   3372 GiB |\n",
            "|       from small pool |    625 KiB |   3100 KiB |    185 GiB |    185 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     598    |     781    |     963 K  |     962 K  |\n",
            "|       from large pool |     285    |     479    |     616 K  |     616 K  |\n",
            "|       from small pool |     313    |     313    |     346 K  |     346 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     598    |     781    |     963 K  |     962 K  |\n",
            "|       from large pool |     285    |     479    |     616 K  |     616 K  |\n",
            "|       from small pool |     313    |     313    |     346 K  |     346 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     241    |     241    |     241    |       0    |\n",
            "|       from large pool |     239    |     239    |     239    |       0    |\n",
            "|       from small pool |       2    |       2    |       2    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      71    |     134    |  660505    |  660434    |\n",
            "|       from large pool |      62    |     123    |  395038    |  394976    |\n",
            "|       from small pool |       9    |      19    |  265467    |  265458    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = qqp_preprocessed['train']\n",
        "val_set = qqp_preprocessed['validation']\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=20, shuffle=True, collate_fn=transformers.default_data_collator, num_workers=4\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=8, shuffle=False, collate_fn=transformers.default_data_collator, num_workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "Y3MKM6WPYtpl",
        "outputId": "888dc600-a2e0-43aa-bb4d-82e6eaa5e730",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_loader, model, loss_fn, optimizer):\n",
        "  losses = []\n",
        "  model.train()\n",
        "  for i, batch in enumerate(tqdm(train_loader)):\n",
        "      batch_on_dev = {k: v.to(device) for k, v in batch.items()}\n",
        "      pred = model(\n",
        "          input_ids=batch_on_dev['input_ids'],\n",
        "          attention_mask=batch_on_dev['attention_mask'],\n",
        "          token_type_ids=batch_on_dev['token_type_ids']\n",
        "      )\n",
        "\n",
        "      loss = loss_fn(pred.logits, batch_on_dev['labels'])\n",
        "      loss.backward()\n",
        "      optimizer.zero_grad()\n",
        "      if i % 500 == 0:\n",
        "          print(f'Curr loss: {loss.item()}')\n",
        "      losses.append(loss.item())\n",
        "  return losses"
      ],
      "metadata": {
        "id": "Lg61cjz_YtmJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def val_epoch(val_loader, model):\n",
        "    preds = []\n",
        "    trues = []\n",
        "    for batch in tqdm(val_loader):\n",
        "        batch_on_dev = {k: v.to(device) for k, v in batch.items()}\n",
        "        pred = model(\n",
        "            input_ids=batch_on_dev['input_ids'],\n",
        "            attention_mask=batch_on_dev['attention_mask'],\n",
        "            token_type_ids=batch_on_dev['token_type_ids']\n",
        "        )\n",
        "        preds_on_dev = torch.argmax(pred.logits, dim=-1).data\n",
        "        trues_on_dev = batch_on_dev['labels'].data\n",
        "        preds.extend(preds_on_dev.cpu().numpy())\n",
        "        trues.extend(trues_on_dev.cpu().numpy())\n",
        "    accuracy = accuracy_score(trues, preds)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "-U22YUEhYtfQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "q1iHawxjYtQR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "losses = []\n",
        "acc = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'Started Epoch no.{epoch}')\n",
        "    epoch_losses = train_epoch(train_loader, model, loss_fn, optimizer)\n",
        "    losses.extend(epoch_losses)\n",
        "\n",
        "    epoch_acc = val_epoch(val_loader, model)\n",
        "    acc.append(epoch_acc)\n",
        "    print(f'Epoch no.{epoch} finished. Loss: {np.mean(epoch_losses)}, Accuracy: {epoch_acc}')"
      ],
      "metadata": {
        "id": "xNqmE9uibk8q",
        "outputId": "291c8201-1404-4ae0-fbf1-a4d7b9d30631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started Epoch no.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/18193 [00:02<12:28:37,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curr loss: 0.6827170252799988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 501/18193 [04:04<2:23:22,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curr loss: 0.6939945220947266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1001/18193 [08:04<2:17:57,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curr loss: 0.7064210176467896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 1501/18193 [12:03<2:14:05,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curr loss: 0.6933838129043579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 1766/18193 [14:11<2:10:55,  2.09it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Закончились лимиты в Google Colab :(**\n",
        "\n",
        "Запустил обучение, пробовал разные размеры батча, но к сожалению бесплатные ресурсы GPU закончились, а также закончилось время до дедлайна. Сдам эту версию и уже после дедлайна попробую поставить обучение снова, думаю с точки зрения идеи обучение адекватное, просто надо больше времени закладывать на этот процесс в следующий раз"
      ],
      "metadata": {
        "id": "iC61HqtH97xt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQD0IV44LrSs"
      },
      "source": [
        "### Task 3: try the full pipeline (1 point)\n",
        "\n",
        "Finally, it is time to use your model to find duplicate questions.\n",
        "Please implement a function that takes a question and finds top-5 potential duplicates in the training set. For now, it is fine if your function is slow, as long as it yields correct results.\n",
        "\n",
        "Showcase how your function works with at least 5 examples."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLSjmsKaUyQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Bonus:__ for bonus points, try to find a way to run the function faster than just passing over all questions in a loop. For isntance, you can form a short-list of potential candidates using a cheaper method, and then run your tranformer on that short list. If you opted for this solution, please keep both the original implementation and the optimized one - and explain briefly what is the difference there."
      ],
      "metadata": {
        "id": "h86vREj7U02k"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1188eeb38ff4bebb8900553aa064cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffa4589b1e874bc0a8d866581b5b2b03",
              "IPY_MODEL_76f378aad1c343cbb7ff6f7c3a1c45a7",
              "IPY_MODEL_b1cda4f37973402db7ebd2af8771a715"
            ],
            "layout": "IPY_MODEL_69a27955d12a41bca267133b2349b700"
          }
        },
        "ffa4589b1e874bc0a8d866581b5b2b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fdec81aa0b740b09976f2075b7ff8b4",
            "placeholder": "​",
            "style": "IPY_MODEL_5db1001a2c0a40d7a41c26f0539d5854",
            "value": "Map: 100%"
          }
        },
        "76f378aad1c343cbb7ff6f7c3a1c45a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32359fae80aa4373afa73d87d4220c2e",
            "max": 390965,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4651eccdeb14299add5325d6dc2da87",
            "value": 390965
          }
        },
        "b1cda4f37973402db7ebd2af8771a715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_566c9622bf144a85b2d2adb6e02e22ca",
            "placeholder": "​",
            "style": "IPY_MODEL_fa6c3f15d17243ffac5f224cd713ba5f",
            "value": " 390965/390965 [01:51&lt;00:00, 4879.94 examples/s]"
          }
        },
        "69a27955d12a41bca267133b2349b700": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fdec81aa0b740b09976f2075b7ff8b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5db1001a2c0a40d7a41c26f0539d5854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32359fae80aa4373afa73d87d4220c2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4651eccdeb14299add5325d6dc2da87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "566c9622bf144a85b2d2adb6e02e22ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa6c3f15d17243ffac5f224cd713ba5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}